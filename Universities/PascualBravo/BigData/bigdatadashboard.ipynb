{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwDUFKaDeaBI"
      },
      "source": [
        "# **   EJERCICIO DE BIG DATA CON LA HERRAMIENTA DE PYTHON EN GOOGLE COLAB:  **\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"El conjunto de datos Online Retail, frecuentemente obtenido del Repositorio de Aprendizaje Automático de la UCI o de plataformas como Kaggle, representa un registro de las transacciones de un minorista en línea con sede en el Reino Unido. Este conjunto de datos abarca un período específico (generalmente desde el 1 de diciembre de 2010 hasta el 9 de diciembre de 2011) y proporciona una instantánea del comportamiento de compra de los clientes de la empresa.\n",
        "\n",
        "\n",
        "\n",
        "La importancia de este dataset radica en su versatilidad para diversos tipos de análisis. En el contexto del archivo que has proporcionado, donde se realiza un ejercicio de clustering K-Means, el dataset permite segmentar a los clientes en grupos basados en sus patrones de compra. Esta segmentación puede ser extremadamente valiosa para la empresa minorista para personalizar sus estrategias de marketing, mejorar la retención de clientes y optimizar la gestión de inventario.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Además del clustering, el dataset Online Retail es útil para:**\n",
        "\n",
        "Análisis de series temporales: Para identificar tendencias de ventas, estacionalidad y realizar pronósticos.\n",
        "\n",
        "\n",
        "Análisis de la cesta de la compra: Para descubrir qué productos se compran juntos con frecuencia, lo que puede informar sobre la colocación de productos y las promociones cruzadas.\n",
        "\n",
        "\n",
        "Análisis geográfico: Para comprender las diferencias en el comportamiento de compra entre los países.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Las características clave de este dataset, que son relevantes para el análisis de clustering y otras tareas, incluyen:\n",
        "\n",
        "InvoiceNo (Identificador de Factura): Representa una transacción única. En el análisis de clustering, podríamos agregar datos a nivel de factura para entender el valor total de la compra o la frecuencia de compra de un cliente.\n",
        "\n",
        "\n",
        "StockCode (Código de Producto): Identifica un producto específico. Si bien no se usa directamente en el clustering típico de clientes, es fundamental para el análisis de productos vendidos dentro de cada segmento de clientes.\n",
        "\n",
        "\n",
        "Quantity (Cantidad): La cantidad de un producto vendido en una transacción. Es una variable clave para segmentar a los clientes según el volumen de compra.\n",
        "\n",
        "\n",
        "UnitPrice (Precio Unitario): El precio de un solo producto. Junto con la cantidad, permite calcular el gasto total del cliente, una variable importante para la segmentación.\n",
        "\n",
        "\n",
        "CustomerID (ID de Cliente): Identifica a un cliente único. El objetivo principal del clustering en este contexto es agrupar a los clientes con características de compra similares.\n",
        "\n",
        "\n",
        "Country (País): El país del cliente. Si bien no se usa directamente en el clustering, permite analizar las diferencias geográficas en los segmentos de clientes.\n",
        "\n",
        "\n",
        "InvoiceDate (Fecha de Factura): Permite el análisis temporal de los segmentos de clientes (por ejemplo, cómo cambian los segmentos a lo largo del tiempo).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Referencias:\n",
        "\n",
        "UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Online+Retail\n",
        "Kaggle (buscar \"Online Retail\"): https://www.kaggle.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrFtfvMOZ-mP"
      },
      "source": [
        "**INSTRUCCIONES:**\n",
        "\n",
        "Por favor siga los pasos indicados para la entrega de este archivo como opción alterna a la entrega propuesta en la plataforma para la evaluación de la Unidad 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVzDmEz9bqAk"
      },
      "source": [
        "1. CARGAR LAS LIBRERIAS PARA EJECUTAR  LOS ALGORITMOS.\n",
        "\n",
        "2. CARGAR LOS 3 ARCHIVOS SUMINISTRADOS A COLAB, EN LA CARPETA LATERAL\n",
        "DONDE SE CARGAN LOS ARCHIVOS DEL ENTORNO DE COLAB datos_20, datos_30 y datos_50 (***) Y LUEGO EJECUTAR LA CELDA PARA INTEGRARLOS.\n",
        "\n",
        "3. EJECUTAR LA CELDA PARA VER UN PROCESO SENCILLO DE  EDA, ANALISIS EXPLORATORIO DE LOS DATOS.\n",
        "\n",
        "4. PARA ESTE PUNTO USTED YA NO TIENE QUE HACER IMPUTACIÓN DE DATOS, SE REALIZA AUTOMATICAMENTE, DE ESTA FORMA DEBE EJECUTAR LA CELDA DE LA IMPUTACION DE DATOS (DATOS NULOS O FALTANTES) EN COLUMNAS  CATEGORIAS Y EN COLUMNAS NUMERICAS, LUEGO INSERTAR LAS RESPUESTAS A LAS PREGUNTAS SOBRE IMPUTACION:\n",
        "\n",
        "4.1.¿EN QUÉ CONSISTE EL PROCESO DE IMPUTACIÓN DE DATOS?\n",
        "\n",
        "4.2.¿CÓMO, CON QUÉ MÉTODO RECOMIENDA USTED RELIZAR UNA IMPUTACIÓN DE DATOS FALTANTES EN UNA COLUMNA DE DATOS CATEGÓRICOS Y EN UNA COLUMNA DE DATOS NUMÉRICOS?\n",
        "\n",
        "ES DECIR QUE USTED SÓLO VA A CONTESTAR ESTAS DOS PREGUNTAS EN ESTE PUNTO NO ES NECESARIO GENERAR PROCESOS DE IMPUTACIÓN DE DATOS.****\n",
        "\n",
        "\n",
        "5. EJECUTAR CELDA PARA LA CLUSTERIZACION (AGRUPACION DE LOS DATOS-CLIENTES ALGORITMO DE MACHINE LEARNING NO SUPERVISADO), CON ESTE SE VA A GENERAR UN NUEVO ARCHIVO CSV CON EL QUE SE VA A TRABAJAR POSTERIORMENTE EN LAS CLASES OBSERVE EL PROCESO.\n",
        "\n",
        "6. HACER E INSERTAR COMO TEXTO O COMO IMAGEN UN CMI CUADRO DE MANDO INTEGRAL PARA LA EMPRESA QUE TIENE ESTOS DATOS (RETAIL, COMO EXITO COMO AMAZON COMO WALMART). USE EL MATERIAL DE LA PLATAFORMA Y EL CONTEXTO DADO AL PRINCIPIO DE ESTE ARCHIVO PARA PODER GENERARLO ****\n",
        "\n",
        "7. GENERAR E INSERTAR COMO TEXTO, TABLA O COMO IMAGEN 5 KPI (ESCOGER 5 KPI QUE SEAN LOS MAS REPRESENTATIVOS PARA LA EMPRESA). USE EL MATERIAL DE LA PLATAFORMA Y EL CONTEXTO DADO AL PRINCIPIO DE ESTE ARCHIVO PARA PODER GENERARLOS ****\n",
        "\n",
        "\n",
        "\n",
        "SON 4 ACTIVIDADES QUE USTED DEBE DESARROLLAR EN ESTA TAREA.(EN LOS PUNTOS 2, 4, 6, 7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gFGE2SjfQQLw"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#  1. CARGAR LAS LIBRERIAS INICIALES PARA EJECUTAR LOS ALGORITMOS. SÓLO DEBE EJECUTAR LA CELDA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "#  1. CARGAR LAS LIBRERIAS INICIALES PARA EJECUTAR LOS ALGORITMOS. SÓLO DEBE EJECUTAR LA CELDA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install dash pandas openpyxl plotly\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqH2I2ZIIVsH"
      },
      "outputs": [],
      "source": [
        "# 2. CARGAR LOS 3 ARCHIVOS SUMINISTRADOS A COLAB, EN LA CARPETA LATERAL\n",
        "# DONDE SE CARGAN LOS ARCHIVOS DEL ENTORNO DE COLAB datos_20, datos_30 y datos_50 (***)\n",
        "# Y LUEGO EJECUTAR LA CELDA PARA INTEGRARLOS.\n",
        "\n",
        "# Reunir los archivos en un solo DataFrame,ya se determinó previamente\n",
        "# que estos archivos se pueden unir porque coiniciden los datos de sus columnas.\n",
        "\n",
        "# Leer los archivos generados\n",
        "df_csv = pd.read_csv(\"/content/datos_20.csv\")\n",
        "df_json = pd.read_json(\"/content/datos_30.json\")\n",
        "df_excel = pd.read_excel(\"/content/datos_50.xlsx\")\n",
        "\n",
        "# Combinar los DataFrames\n",
        "df_combinados = pd.concat([df_csv, df_json, df_excel], ignore_index=True)\n",
        "\n",
        "# Verificar que los datos combinados sean correctos\n",
        "print(\"\\nDatos combinados después de reunir los archivos:\")\n",
        "print(df_combinados.info())\n",
        "\n",
        "print(\"\\nVerificación completada: Todos los pasos se han realizado correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGMlVp82LE_8"
      },
      "outputs": [],
      "source": [
        "#3. EJECUTAR LA CELDA PARA VER UN PROCESO SENCILLO DE EDA, ANALISIS EXPLORATORIO DE LOS DATOS.\n",
        "\n",
        "# Cargar el dataset\n",
        "\n",
        "df =  df_combinados\n",
        "\n",
        "# Mostrar información inicial del dataset\n",
        "print(\"Información inicial del dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Mostrar las primeras filas\n",
        "print(\"\\nPrimeras filas del dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "#  Convertir datos de las fechas a datetime\n",
        "\n",
        "try:\n",
        "    # Verificar si los valores son números grandes (probablemente timestamps en milisegundos)\n",
        "    # El tipo de dato podría ser 'object' al provenir de fuentes combinadas\n",
        "    # y contener representaciones numéricas y de texto de las fechas.\n",
        "    # Intentemos convertir a numérico primero, manejando errores:\n",
        "    df['InvoiceDate'] = pd.to_numeric(df['InvoiceDate'], errors='coerce')\n",
        "    if df['InvoiceDate'].dtype in ['int64', 'float64']:\n",
        "        # Convertir de milisegundos a datetime\n",
        "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], unit='ms')\n",
        "    else:\n",
        "        # Intentar convertir directamente a datetime, usando format='mixed'\n",
        "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='raise', format='mixed')\n",
        "except ValueError as e:\n",
        "    print(f\"Error al convertir InvoiceDate: {e}\")\n",
        "    print(\"Verifica el formato de los datos en la columna 'InvoiceDate'.\")\n",
        "    raise\n",
        "\n",
        "# Mostrar las primeras filas después de la conversión\n",
        "print(\"\\nPrimeras filas después de convertir InvoiceDate:\")\n",
        "print(df.head())\n",
        "\n",
        "#  Estadísticas Descriptivas\n",
        "\n",
        "# Resumen numérico\n",
        "print(\"\\nResumen estadístico de variables numéricas:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Resumen de variables categóricas\n",
        "print(\"\\nResumen de variables categóricas:\")\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\nDistribución de {col}:\")\n",
        "    print(df[col].value_counts())\n",
        "\n",
        "# Visualización de Datos\n",
        "# Configuración de estilo\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Histograma de Quantity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Quantity'], bins=50, kde=False, color='blue')\n",
        "plt.title(\"Distribución de Quantity\")\n",
        "plt.xlabel(\"Quantity\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()\n",
        "\n",
        "# Histograma de UnitPrice\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df[df['UnitPrice'] > 0]['UnitPrice'], bins=50, kde=False, color='green')\n",
        "plt.title(\"Distribución de UnitPrice (Valores > 0)\")\n",
        "plt.xlabel(\"UnitPrice\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplot de Quantity para detectar outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=df['Quantity'])\n",
        "plt.title(\"Boxplot de Quantity\")\n",
        "plt.xlabel(\"Quantity\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplot de UnitPrice para detectar outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=df[df['UnitPrice'] > 0]['UnitPrice'])\n",
        "plt.title(\"Boxplot de UnitPrice (Valores > 0)\")\n",
        "plt.xlabel(\"UnitPrice\")\n",
        "plt.show()\n",
        "\n",
        "# Distribución de datos por países\n",
        "plt.figure(figsize=(12, 8))\n",
        "country_counts = df['Country'].value_counts()\n",
        "sns.barplot(x=country_counts[:10].index, y=country_counts[:10].values, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Países por Número de Transacciones\")\n",
        "plt.xlabel(\"País\")\n",
        "plt.ylabel(\"Número de Transacciones\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Análisis Temporal\n",
        "# Extraer año y mes\n",
        "df['Year'] = df['InvoiceDate'].dt.year\n",
        "df['Month'] = df['InvoiceDate'].dt.month\n",
        "\n",
        "# Ventas mensuales\n",
        "monthly_sales = df.groupby(['Year', 'Month'])['Quantity'].sum().reset_index()\n",
        "monthly_sales['Year-Month'] = monthly_sales['Year'].astype(str) + '-' + monthly_sales['Month'].astype(str)\n",
        "\n",
        "# Gráfico de ventas mensuales\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=monthly_sales, x='Year-Month', y='Quantity', marker='o')\n",
        "plt.title(\"Ventas Mensuales\")\n",
        "plt.xlabel(\"Año-Mes\")\n",
        "plt.ylabel(\"Cantidad Vendida\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Análisis de Correlaciones\n",
        "# Calcular ingresos totales\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Matriz de correlación\n",
        "correlation_matrix = df[['Quantity', 'UnitPrice', 'TotalPrice']].corr()\n",
        "\n",
        "# Mapa de calor de correlaciones\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Matriz de Correlación\")\n",
        "plt.show()\n",
        "\n",
        "# Análisis de Outliers Valores atípicos\n",
        "# Identificar outliers en Quantity usando IQR\n",
        "Q1 = df['Quantity'].quantile(0.25)\n",
        "Q3 = df['Quantity'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers_quantity = df[(df['Quantity'] < lower_bound) | (df['Quantity'] > upper_bound)]\n",
        "print(f\"\\nNúmero de outliers en Quantity: {len(outliers_quantity)}\")\n",
        "\n",
        "# Identificar outliers en UnitPrice usando IQR\n",
        "Q1 = df['UnitPrice'].quantile(0.25)\n",
        "Q3 = df['UnitPrice'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers_unitprice = df[(df['UnitPrice'] < lower_bound) | (df['UnitPrice'] > upper_bound)]\n",
        "print(f\"Número de outliers en UnitPrice: {len(outliers_unitprice)}\")\n",
        "\n",
        "# Conclusión del EDA\n",
        "print(\"\\nConclusión del Análisis Exploratorio:\")\n",
        "print(\"\"\"\n",
        "1. El dataset contiene transacciones de ventas minoristas con detalles como cantidad, precio unitario, fecha, país, etc.\n",
        "2. Las variables numéricas (Quantity y UnitPrice) tienen distribuciones sesgadas y presentan valores atípicos significativos.\n",
        "3. El Reino Unido es el país con más transacciones, seguido de otros países europeos.\n",
        "4. Las ventas muestran una tendencia estacional, con picos en ciertos meses.\n",
        "5. Existe una débil correlación entre Quantity y TotalPrice, lo que sugiere que otros factores influyen en los ingresos.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTtKVp_-TMd0"
      },
      "outputs": [],
      "source": [
        "#  4. PARA ESTE PUNTO USTED YA NO TIENE QUE HACER IMPUTACIÓN DE DATOS, SE REALIZA AUTOMATICAMENTE,\n",
        "#  DE ESTA FORMA DEBE EJECUTAR LA CELDA DE LA IMPUTACION DE DATOS (DATOS NULOS O FALTANTES)\n",
        "#  EN COLUMNAS  CATEGORIAS Y EN COLUMNAS NUMERICAS, LUEGO INSERTAR LAS RESPUESTAS A LAS PREGUNTAS SOBRE IMPUTACION:\n",
        "\n",
        "#  Revisar datos faltantes\n",
        "print(\"\\nRevisión de datos faltantes:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Mostrar porcentaje de valores faltantes por columna\n",
        "print(\"\\nPorcentaje de datos faltantes por columna:\")\n",
        "print((df.isnull().sum() / len(df)) * 100)\n",
        "\n",
        "# Imputación de datos faltantes para variables categóricas\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "# Imputación de datos faltantes para variables numéricas\n",
        "for col in df.select_dtypes(include=['number']).columns:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Verificar la imputación\n",
        "print(\"\\nRevisión de datos faltantes después de la imputación:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nPorcentaje de datos faltantes por columna después de la imputación:\")\n",
        "print((df.isnull().sum() / len(df)) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlhK608jh2eW"
      },
      "source": [
        "INSERTAR AQUI LAS RESPUESTAS DE:\n",
        "\n",
        "¿EN QUÉ CONSISTE EL PROCESO DE IMPUTACIÓN DE DATOS?\n",
        "\n",
        "¿CÓMO, CON QUÉ MÉTODO RECOMIENDA USTED RELIZAR UNA IMPUTACIÓN DE DATOS FALTANTES EN UNA COLUMNA DE DATOS CATEGÓRICOS Y EN UNA COLUMNA DE DATOS NUMÉRICOS?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvC2Ox65ZhWe"
      },
      "outputs": [],
      "source": [
        "#   5. EJECUTAR CELDA PARA LA CLUSTERIZACION (AGRUPACION DE LOS DATOS-CLIENTES ALGORITMO\n",
        "#   DE MACHINE LEARNING NO SUPERVISADO), CON ESTE SE VA A GENERAR UN NUEVO ARCHIVO CSV\n",
        "#   CON EL QUE SE VA A TRABAJAR POSTERIORMENTE EN LAS CLASES OBSERVE EL PROCESO.\n",
        "\n",
        "\n",
        "\n",
        "# Preparación de datos para K-Means\n",
        "print(\"\\n---  Preparación de datos para K-Means ---\")\n",
        "\n",
        "# Seleccionar las columnas relevantes para la clusterización\n",
        "features = ['Quantity', 'UnitPrice', 'TotalPrice']  # Puedes ajustar las columnas según tus necesidades\n",
        "print(f\"Características seleccionadas para la clusterización: {features}\")\n",
        "\n",
        "# Crear un nuevo DataFrame con las características seleccionadas\n",
        "X = df[features].copy() # Usamos .copy() para evitar SettingWithCopyWarning\n",
        "\n",
        "# Convertir las columnas a tipo numérico si es necesario\n",
        "print(\"\\nVerificando y convirtiendo las características a tipo numérico:\")\n",
        "for feature in features:\n",
        "    print(f\"  - Tipo de dato de '{feature}' antes de la conversión: {X[feature].dtype}\")\n",
        "    X[feature] = pd.to_numeric(X[feature], errors='coerce')  # 'coerce' convertirá valores no numéricos a NaN\n",
        "    print(f\"  - Tipo de dato de '{feature}' después de la conversión: {X[feature].dtype}\")\n",
        "\n",
        "# Identificar y manejar valores NaN introducidos por la conversión\n",
        "print(\"\\nIdentificando y eliminando filas con valores NaN después de la conversión:\")\n",
        "nan_rows_before = X.isnull().any(axis=1).sum()\n",
        "print(f\"  - Número de filas con NaN antes de eliminar: {nan_rows_before}\")\n",
        "X = X.dropna()\n",
        "print(f\"  - Número de filas con NaN después de eliminar: {X.isnull().any(axis=1).sum()}\")\n",
        "print(f\"  - Número de filas en el DataFrame de características después de limpiar: {len(X)}\")\n",
        "\n",
        "# Escalar los datos para que todas las características tengan la misma importancia\n",
        "print(\"\\nEscalando las características para K-Means:\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"  - Datos escalados (primeras 5 filas):\\n\", X_scaled[:5])\n",
        "\n",
        "#  Ejecutar K-Means\n",
        "print(\"\\n---  Ejecutar K-Means ---\")\n",
        "\n",
        "# Determinar el número óptimo de clusters (k) usando el método del codo\n",
        "wcss = []\n",
        "range_clusters = range(1, 11)\n",
        "print(f\"Calculando la Suma de Cuadrados Intra-Cluster (WCSS) para un rango de clusters: {range_clusters}\")\n",
        "for i in range_clusters:\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init='auto', random_state=0) # Corrección: Eliminar la repetición de n_init\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Graficar el método del codo\n",
        "print(\"\\nGraficando el método del codo para encontrar el número óptimo de clusters:\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range_clusters, wcss, marker='o', linestyle='--')\n",
        "plt.title('Método del Codo')\n",
        "plt.xlabel('Número de Clusters (k)')\n",
        "plt.ylabel('Suma de Cuadrados Intra-Cluster (WCSS)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Elegir el número óptimo de clusters (k) según el gráfico del método del codo.\n",
        "\n",
        "# Observa el gráfico y busca el punto donde la disminución de WCSS se vuelve menos pronunciada (el \"codo\").\n",
        "optimal_k = 5  # ¡IMPORTANTE! se ajusta este valor basándose en la gráfica del codo.\n",
        "print(f\"\\nNúmero óptimo de clusters seleccionado (basado en la observación del gráfico del codo): {optimal_k}\")\n",
        "\n",
        "# Aplicar K-Means con el número óptimo de clusters\n",
        "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init='auto', random_state=0)\n",
        "df_temp = df[features].dropna().copy() # Asegurarse de usar solo las filas sin NaN para la predicción\n",
        "df_temp_scaled = scaler.transform(df_temp) # Escalar usando el mismo scaler ajustado previamente\n",
        "clusters = kmeans.fit_predict(df_temp_scaled)\n",
        "\n",
        "# Asignación de clusters solo a las filas donde no había NaN en las características\n",
        "df.loc[df[features].dropna().index, 'Cluster'] = clusters\n",
        "\n",
        "# Llenar los posibles NaN en la columna 'Cluster' con un valor indicativo (opcional)\n",
        "df['Cluster'].fillna(-1, inplace=True) # -1 indica que la fila no se incluyó en la clusterización debido a NaN\n",
        "\n",
        "print(\"\\nSe han asignado los clusters al DataFrame original.\")\n",
        "print(\"\\nPrimeras filas del DataFrame con la columna 'Cluster':\")\n",
        "print(df.head())\n",
        "\n",
        "#  Análisis de Clusters\n",
        "print(\"\\n---  Análisis de Clusters ---\")\n",
        "print(\"\\nEstadísticas descriptivas de cada cluster:\")\n",
        "\n",
        "# ¡CORRECCIÓN IMPORTANTE! Agrupar solo por las filas que tienen un cluster asignado (no NaN)\n",
        "print(df[df['Cluster'] != -1].groupby('Cluster')[features].agg(['mean', 'median']))\n",
        "\n",
        "# Visualizar los clusters (ejemplo con las primeras dos características escaladas)\n",
        "print(\"\\nVisualización de los clusters (usando las dos primeras características escaladas):\")\n",
        "if len(features) >= 2:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Solo se grafican los puntos que fueron clusterizados\n",
        "    X_clustered = X_scaled[~df['Cluster'].isna()]\n",
        "    clusters_assigned = df['Cluster'][~df['Cluster'].isna()].astype(int) # Convertir a int para el color\n",
        "\n",
        "    scatter = plt.scatter(X_clustered[:, 0], X_clustered[:, 1], c=clusters_assigned, cmap='viridis', label='Datos')\n",
        "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', label='Centroides')\n",
        "    plt.title('Clusters de Datos (Características Escaladas)')\n",
        "    plt.xlabel(f'Característica Escalada: {features[0]}')\n",
        "    plt.ylabel(f'Característica Escalada: {features[1]}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Crear una leyenda para los colores de los clusters\n",
        "    legend1 = plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "    plt.gca().add_artist(legend1)\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No se pueden visualizar los clusters con menos de dos características seleccionadas.\")\n",
        "\n",
        "# Guardar el DataFrame con los clusters en un nuevo archivo CSV\n",
        "print(\"\\n---  Guardar el DataFrame con los clusters ---\")\n",
        "nombre_archivo_csv = \"datosconclusters.csv\"\n",
        "df.to_csv(nombre_archivo_csv, index=False)\n",
        "\n",
        "print(f\"\\nEl DataFrame con la columna 'Cluster' ha sido guardado en el archivo: {nombre_archivo_csv}\")\n",
        "\n",
        "print(\"\\n¡El código se ha ejecutado sin problemas y se han generado los clusters!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_-umxgUeJj"
      },
      "source": [
        "6. HACER E INSERTAR COMO TEXTO O COMO IMAGEN UN CMI CUADRO DE MANDO INTEGRAL PARA LA EMPRESA QUE TIENE ESTOS DATOS (RETAIL, COMO EXITO COMO AMAZON COMO WALMART). USE EL MATERIAL DE LA PLATAFORMA Y EL CONTEXTO DADO AL PRINCIPIO DE ESTE ARCHIVO PARA PODER GENERARLO ****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KQEBVk9XHrV"
      },
      "source": [
        "7. GENERAR E INSERTAR COMO TEXTO, TABLA O COMO IMAGEN 5 KPI (ESCOGER 5 KPI QUE SEAN LOS MAS REPRESENTATIVOS PARA LA EMPRESA). USE EL MATERIAL DE LA PLATAFORMA Y EL CONTEXTO DADO AL PRINCIPIO DE ESTE ARCHIVO PARA PODER GENERARLOS ****"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
